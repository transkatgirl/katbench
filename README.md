# katbench
what i use to compare LLM base models

## setup

> [!IMPORTANT]
> due to a [bug in lighteval](https://github.com/huggingface/lighteval/pull/502)'s current TGI implementation, you will need to use the [transkatgirl/lighteval](https://github.com/transkatgirl/lighteval) fork

you will need:

- a working [TGI server](https://huggingface.co/docs/text-generation-inference/en/index) run with the `--enable-prefill-logprobs --payload-limit 100000000` flags
- [lighteval](https://huggingface.co/docs/lighteval/index) w/ `tgi,math` extras

## usage

first, run the `run.sh` file. this will create the `model.yaml` and `tasks.txt` files before returning an error.

update the `model.yaml` file with your TGI server endpoint, and update the `tasks.txt` file if you wish to change the list of tasks to run. then, run the `run.sh` file to begin the benchmark.

> [!WARNING]
> i have not yet evaluated lighteval's handling of documents larger than the LLM's context window. as a result, i'd currently recommend against comparing results from these scripts against those generated by other benchmarking tools using the same datasets.