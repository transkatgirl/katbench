# katbench
what i use to compare LLM base models

this is a work in progress; i plan on adding a lot more to it over time.

> [!WARNING]
> THIS IS CURRENTLY BROKEN, and outputs nonsensical results. i plan on rewriting it soon.

## setup

> [!IMPORTANT]
> due to a [bug in lighteval](https://github.com/huggingface/lighteval/pull/502)'s current TGI implementation, you will need to use the [transkatgirl/lighteval](https://github.com/transkatgirl/lighteval) fork

you will need:

- a working [TGI server](https://huggingface.co/docs/text-generation-inference/en/index) run with the `--enable-prefill-logprobs --payload-limit 100000000` flags
- [lighteval](https://huggingface.co/docs/lighteval/index) w/ `tgi,math` extras

## usage

first, run the `run.sh` file. this will create the `model.yaml` and `tasks.txt` files before returning an error.

update the `model.yaml` file with your TGI server endpoint, and update the `tasks.txt` file if you wish to change the list of tasks to run. then, run the `run.sh` file to begin the benchmark.

results will be output to the `output/results` folder, organized into folders by model name.

> [!WARNING]
> at the moment, dataset documents larger than the context window are truncated. as a result, i'd currently recommend against comparing results from these scripts against those generated by other benchmarking tools using the same datasets.

## result analysis

TODO